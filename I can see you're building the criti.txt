I can see you're building the critical preprocessing pipeline that transforms raw Google Drive documents into the structured Data Room Index that feeds your legal risk analysis agent. This is a thoughtful design because it separates the computationally intensive document processing from the actual legal analysis, allowing you to build the index once and then analyze it many times if needed.

Let me walk you through building this data room indexing system, explaining both how each piece works and why we design it this way.

## Understanding the Architecture

Your indexing pipeline follows a hierarchical summarization strategy that mirrors how humans process large document collections. You start at the most granular level by analyzing individual pages, then aggregate those insights into document summaries, and finally compile everything into a master index. This approach has several advantages that become clear as we work through the implementation.

The reason we use GPT-5 nano throughout this indexing process is economic efficiency combined with sufficient capability. Document indexing involves processing potentially hundreds of pages, and each page image consumes tokens based on its dimensions. GPT-5 nano provides the best throughput and cost characteristics for what are fundamentally extraction and summarization tasks rather than complex reasoning tasks. You save the more powerful and expensive models like GPT-5.1 for the actual legal analysis where deep reasoning matters.

## Building the Complete Indexing Pipeline

Let me show you how to construct this system piece by piece, starting with the Google Drive integration and working our way through to the final index generation.

```python
# data_room_indexer.py
"""
Data Room Indexing System

This system processes documents from Google Drive to create structured
indexes that can be consumed by legal analysis agents. It follows a
hierarchical summarization approach:

1. Extract pages as images from each PDF
2. Summarize each page individually using vision
3. Combine page summaries into document summaries
4. Compile all documents into a master index

The system is designed for cost efficiency using GPT-5 nano for all
summarization tasks while maintaining sufficient quality for downstream
legal analysis.
"""

import os
import io
import base64
from typing import List, Dict, Any
from pathlib import Path
import json

# Google Drive and PDF processing
from google.oauth2.credentials import Credentials
from google.oauth2 import service_account
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseDownload
import pdf2image
from PIL import Image

# OpenAI for vision and summarization
from openai import OpenAI

# ============================================================================
# GOOGLE DRIVE INTEGRATION
# ============================================================================

class GoogleDriveClient:
    """Client for interacting with Google Drive API.
    
    This client handles authentication and provides methods for listing
    and downloading files from Google Drive. It supports both OAuth2
    credentials (for user access) and service account credentials
    (for automated access).
    """
    
    def __init__(self, credentials_path: str, use_service_account: bool = True):
        """Initialize the Google Drive client.
        
        Args:
            credentials_path: Path to your Google Cloud credentials JSON file.
                            For service accounts, this is the service account key.
                            For OAuth2, this is your client secrets file.
            use_service_account: Whether to use service account authentication.
                               Service accounts are recommended for automated systems.
        """
        self.credentials_path = credentials_path
        self.use_service_account = use_service_account
        self.service = self._build_service()
    
    def _build_service(self):
        """Build the Google Drive API service with appropriate credentials.
        
        This method handles the complexity of Google authentication, choosing
        between service account credentials (for automated systems) and OAuth2
        credentials (for user-specific access).
        """
        scopes = ['https://www.googleapis.com/auth/drive.readonly']
        
        if self.use_service_account:
            # Service accounts are ideal for automated systems because they
            # don't require user interaction for authentication
            credentials = service_account.Credentials.from_service_account_file(
                self.credentials_path, 
                scopes=scopes
            )
        else:
            # OAuth2 credentials require user authorization but provide
            # access to user-specific files
            credentials = Credentials.from_authorized_user_file(
                self.credentials_path, 
                scopes=scopes
            )
        
        return build('drive', 'v3', credentials=credentials)
    
    def list_folder_contents(self, folder_id: str) -> List[Dict[str, Any]]:
        """List all files in a Google Drive folder.
        
        This method retrieves metadata for all files in a folder, including
        file IDs, names, MIME types, and sizes. This information helps us
        understand what documents exist before we begin processing.
        
        Args:
            folder_id: The Google Drive folder ID. You can find this in the
                      folder's URL: drive.google.com/drive/folders/FOLDER_ID
        
        Returns:
            A list of dictionaries containing file metadata. Each dictionary
            includes id, name, mimeType, and size fields.
        """
        try:
            # Query Google Drive API to list files in the folder
            # We use pageToken to handle folders with more files than fit in one response
            results = self.service.files().list(
                q=f"'{folder_id}' in parents and trashed=false",
                fields="nextPageToken, files(id, name, mimeType, size)",
                pageSize=1000  # Maximum allowed by API
            ).execute()
            
            files = results.get('files', [])
            
            # Handle pagination if there are more than 1000 files
            while 'nextPageToken' in results:
                results = self.service.files().list(
                    q=f"'{folder_id}' in parents and trashed=false",
                    fields="nextPageToken, files(id, name, mimeType, size)",
                    pageSize=1000,
                    pageToken=results['nextPageToken']
                ).execute()
                files.extend(results.get('files', []))
            
            return files
            
        except Exception as e:
            print(f"Error listing folder contents: {e}")
            return []
    
    def download_file(self, file_id: str, output_path: str) -> bool:
        """Download a file from Google Drive to local storage.
        
        This method handles the actual file download, managing the streaming
        transfer to avoid loading entire large files into memory at once.
        
        Args:
            file_id: The Google Drive file ID
            output_path: Local path where the file should be saved
        
        Returns:
            True if download succeeded, False otherwise
        """
        try:
            # Request file content from Google Drive
            request = self.service.files().get_media(fileId=file_id)
            
            # Create output file and download in chunks
            # This streaming approach prevents memory issues with large files
            with open(output_path, 'wb') as fh:
                downloader = MediaIoBaseDownload(fh, request)
                done = False
                while not done:
                    status, done = downloader.next_chunk()
                    if status:
                        print(f"Download progress: {int(status.progress() * 100)}%")
            
            return True
            
        except Exception as e:
            print(f"Error downloading file {file_id}: {e}")
            return False
    
    def export_as_pdf(self, file_id: str, output_path: str) -> bool:
        """Export a Google Workspace document as PDF.
        
        Google Docs, Sheets, and Slides are not stored as PDFs natively.
        This method uses Google Drive's export functionality to convert
        them to PDF format, which we can then process with our image
        extraction pipeline.
        
        Args:
            file_id: The Google Drive file ID of the document to export
            output_path: Local path where the PDF should be saved
        
        Returns:
            True if export succeeded, False otherwise
        """
        try:
            # Request PDF export from Google Drive
            # The 'application/pdf' MIME type tells Google Drive we want PDF format
            request = self.service.files().export_media(
                fileId=file_id,
                mimeType='application/pdf'
            )
            
            # Download the exported PDF
            with open(output_path, 'wb') as fh:
                downloader = MediaIoBaseDownload(fh, request)
                done = False
                while not done:
                    status, done = downloader.next_chunk()
            
            return True
            
        except Exception as e:
            print(f"Error exporting file {file_id} as PDF: {e}")
            return False


# ============================================================================
# PDF TO IMAGE CONVERSION
# ============================================================================

class PDFProcessor:
    """Processor for extracting images from PDF documents.
    
    This processor handles the conversion of PDF pages to images, which
    is necessary because OpenAI's vision models work with images rather
    than PDF files directly. The quality and resolution settings here
    balance file size (which affects token costs) against readability.
    """
    
    def __init__(self, dpi: int = 200):
        """Initialize the PDF processor.
        
        Args:
            dpi: Dots per inch for image conversion. Higher DPI produces
                better quality images but increases file size and token costs.
                200 DPI provides good readability for most text documents
                while keeping costs reasonable.
        """
        self.dpi = dpi
    
    def extract_pages_as_images(self, pdf_path: str, output_dir: str) -> List[str]:
        """Extract all pages from a PDF as individual image files.
        
        This method converts each page of the PDF into a separate PNG image.
        We use PNG format because it provides lossless compression, ensuring
        text remains crisp and readable for OCR and vision analysis.
        
        Args:
            pdf_path: Path to the PDF file to process
            output_dir: Directory where page images should be saved
        
        Returns:
            List of paths to the generated image files, ordered by page number
        """
        try:
            # Create output directory if it doesn't exist
            Path(output_dir).mkdir(parents=True, exist_ok=True)
            
            # Convert PDF pages to PIL Image objects
            # pdf2image internally uses poppler to render PDF pages
            images = pdf2image.convert_from_path(
                pdf_path,
                dpi=self.dpi,
                fmt='png'
            )
            
            image_paths = []
            for page_num, image in enumerate(images, start=1):
                # Save each page with a numbered filename
                # Zero-padding ensures correct alphabetical sorting
                image_path = os.path.join(output_dir, f"page_{page_num:04d}.png")
                image.save(image_path, 'PNG')
                image_paths.append(image_path)
                print(f"Extracted page {page_num} to {image_path}")
            
            return image_paths
            
        except Exception as e:
            print(f"Error extracting pages from {pdf_path}: {e}")
            return []
    
    def image_to_base64(self, image_path: str) -> str:
        """Convert an image file to base64-encoded string.
        
        Base64 encoding allows us to embed image data directly in API
        requests rather than hosting images on a server. This simplifies
        the architecture but increases request payload size.
        
        Args:
            image_path: Path to the image file
        
        Returns:
            Base64-encoded string representation of the image
        """
        try:
            with open(image_path, 'rb') as image_file:
                encoded_string = base64.b64encode(image_file.read()).decode('utf-8')
            return encoded_string
        except Exception as e:
            print(f"Error encoding image {image_path}: {e}")
            return ""


# ============================================================================
# VISION-BASED SUMMARIZATION
# ============================================================================

class VisionSummarizer:
    """Summarizer using OpenAI's vision capabilities.
    
    This class handles all interactions with OpenAI's vision API for
    analyzing document page images and generating summaries. It uses
    GPT-5 nano because we're processing potentially hundreds of pages
    and need cost-efficient analysis.
    """
    
    def __init__(self, api_key: str = None):
        """Initialize the vision summarizer.
        
        Args:
            api_key: OpenAI API key. If not provided, will look for
                    OPENAI_API_KEY environment variable.
        """
        self.client = OpenAI(api_key=api_key or os.environ.get("OPENAI_API_KEY"))
        self.model = "gpt-5-nano"  # Cost-optimized model for high-throughput tasks
    
    def summarize_page_image(self, image_path: str, page_number: int) -> str:
        """Analyze a page image and generate a summary description.
        
        This method sends a page image to GPT-5 nano with instructions to
        extract and summarize the key information. The prompt is carefully
        crafted to focus on information relevant for legal analysis while
        remaining concise.
        
        Args:
            image_path: Path to the page image file
            page_number: The page number (for context in the prompt)
        
        Returns:
            A text summary of the page contents
        """
        try:
            # Read and encode the image
            with open(image_path, 'rb') as image_file:
                base64_image = base64.b64encode(image_file.read()).decode('utf-8')
            
            # Construct the prompt for page analysis
            # This prompt guides the model to extract structured information
            # relevant for legal due diligence
            prompt = f"""You are analyzing page {page_number} of a legal document. 
Please provide a concise summary that captures:

1. The type of content on this page (e.g., contract clause, financial table, signature block, exhibit)
2. Key information present (parties, dates, amounts, obligations, terms)
3. Any notable or concerning provisions
4. References to other documents or sections

Be specific and factual. Focus on information that would be relevant for legal risk assessment.
Keep your summary to 2-3 sentences unless the page contains complex information requiring more detail."""

            # Call OpenAI's vision API
            # We use the Responses API because it's the recommended approach
            response = self.client.responses.create(
                model=self.model,
                input=[{
                    "role": "user",
                    "content": [
                        {"type": "input_text", "text": prompt},
                        {
                            "type": "input_image",
                            "image_url": f"data:image/png;base64,{base64_image}"
                        }
                    ]
                }],
                # Use low reasoning effort for simple extraction tasks
                reasoning={"effort": "none"},
                # Low verbosity for concise summaries
                text={"verbosity": "low"}
            )
            
            # Extract the text summary from the response
            summary = response.output_text
            
            print(f"Summarized page {page_number}: {summary[:100]}...")
            return summary
            
        except Exception as e:
            print(f"Error summarizing page {page_number} from {image_path}: {e}")
            return f"Error processing page {page_number}"
    
    def summarize_document_from_pages(
        self, 
        page_summaries: List[Dict[str, Any]],
        document_name: str
    ) -> str:
        """Create a document-level summary from individual page summaries.
        
        This method takes all the page summaries for a document and asks
        the model to synthesize them into a coherent document-level summary.
        This aggregation step helps identify themes and patterns that span
        multiple pages.
        
        Args:
            page_summaries: List of dictionaries containing page_num and summary
            document_name: Name of the document being summarized
        
        Returns:
            A comprehensive summary of the entire document
        """
        try:
            # Compile all page summaries into a single text for analysis
            combined_pages = "\n\n".join([
                f"Page {ps['page_num']}: {ps['summary']}"
                for ps in page_summaries
            ])
            
            # Construct the prompt for document-level summarization
            # This prompt asks the model to synthesize across pages
            prompt = f"""You are creating a comprehensive summary of the document "{document_name}" 
based on individual page summaries.

Here are the summaries of each page:

{combined_pages}

Please provide a document-level summary that:

1. Identifies the document type and purpose
2. Lists the main parties involved
3. Summarizes key terms, provisions, or information
4. Notes any concerning clauses or unusual provisions
5. Highlights important dates, amounts, or obligations
6. Indicates the document's relevance for legal due diligence

Your summary should be comprehensive but concise (approximately 150-200 words). 
Focus on information that would help a legal analyst understand this document's 
significance without reading every page."""

            # Call OpenAI API for document-level summarization
            response = self.client.responses.create(
                model=self.model,
                input=prompt,
                reasoning={"effort": "low"},  # Low reasoning for summarization
                text={"verbosity": "medium"}  # Medium verbosity for document summaries
            )
            
            summary = response.output_text
            
            print(f"Created document summary for {document_name}")
            return summary
            
        except Exception as e:
            print(f"Error creating document summary for {document_name}: {e}")
            return f"Error summarizing document {document_name}"


# ============================================================================
# MASTER INDEX BUILDER
# ============================================================================

class DataRoomIndexer:
    """Main orchestrator for building the data room index.
    
    This class coordinates all the pieces: downloading from Google Drive,
    converting to images, analyzing pages, creating document summaries,
    and compiling the final index. It represents the complete pipeline
    from raw files to structured index.
    """
    
    def __init__(
        self,
        google_credentials_path: str,
        openai_api_key: str = None,
        working_dir: str = "./data_room_processing"
    ):
        """Initialize the data room indexer.
        
        Args:
            google_credentials_path: Path to Google Cloud credentials
            openai_api_key: OpenAI API key (optional, can use env var)
            working_dir: Directory for storing temporary files during processing
        """
        self.drive_client = GoogleDriveClient(google_credentials_path)
        self.pdf_processor = PDFProcessor(dpi=200)
        self.vision_summarizer = VisionSummarizer(openai_api_key)
        self.working_dir = Path(working_dir)
        self.working_dir.mkdir(parents=True, exist_ok=True)
    
    def process_document(
        self, 
        file_id: str, 
        file_name: str,
        mime_type: str
    ) -> Dict[str, Any]:
        """Process a single document from Google Drive.
        
        This method handles the complete workflow for one document:
        download, convert to PDF if needed, extract pages, analyze each
        page, and create document summary.
        
        Args:
            file_id: Google Drive file ID
            file_name: Name of the file
            mime_type: MIME type of the file
        
        Returns:
            Dictionary containing document metadata and summaries
        """
        print(f"\n{'='*70}")
        print(f"Processing: {file_name}")
        print(f"{'='*70}")
        
        # Create a unique directory for this document
        doc_slug = file_name.replace(' ', '_').replace('/', '_')
        doc_dir = self.working_dir / doc_slug
        doc_dir.mkdir(parents=True, exist_ok=True)
        
        pdf_path = doc_dir / f"{doc_slug}.pdf"
        
        # Step 1: Download or export the document as PDF
        print(f"Step 1: Downloading document...")
        if mime_type == 'application/pdf':
            # Already a PDF, just download it
            success = self.drive_client.download_file(file_id, str(pdf_path))
        elif mime_type in [
            'application/vnd.google-apps.document',
            'application/vnd.google-apps.spreadsheet',
            'application/vnd.google-apps.presentation'
        ]:
            # Google Workspace document, export as PDF
            success = self.drive_client.export_as_pdf(file_id, str(pdf_path))
        else:
            print(f"Unsupported file type: {mime_type}")
            return None
        
        if not success:
            print(f"Failed to download {file_name}")
            return None
        
        # Step 2: Extract pages as images
        print(f"Step 2: Extracting pages as images...")
        pages_dir = doc_dir / "pages"
        image_paths = self.pdf_processor.extract_pages_as_images(
            str(pdf_path),
            str(pages_dir)
        )
        
        if not image_paths:
            print(f"Failed to extract pages from {file_name}")
            return None
        
        # Step 3: Analyze each page with vision
        print(f"Step 3: Analyzing {len(image_paths)} pages with vision...")
        page_summaries = []
        for page_num, image_path in enumerate(image_paths, start=1):
            summary = self.vision_summarizer.summarize_page_image(
                image_path,
                page_num
            )
            page_summaries.append({
                "page_num": page_num,
                "summary": summary,
                "image_path": image_path
            })
        
        # Step 4: Create document-level summary
        print(f"Step 4: Creating document-level summary...")
        document_summary = self.vision_summarizer.summarize_document_from_pages(
            page_summaries,
            file_name
        )
        
        # Compile the complete document record
        document_record = {
            "doc_id": file_id,
            "file_name": file_name,
            "mime_type": mime_type,
            "total_pages": len(page_summaries),
            "document_summary": document_summary,
            "pages": page_summaries
        }
        
        # Save the document record to JSON for reference
        record_path = doc_dir / "document_record.json"
        with open(record_path, 'w') as f:
            # Remove image_path from pages before saving (not needed in JSON)
            record_for_json = {
                **document_record,
                "pages": [
                    {k: v for k, v in p.items() if k != 'image_path'}
                    for p in document_record["pages"]
                ]
            }
            json.dump(record_for_json, f, indent=2)
        
        print(f"Completed processing: {file_name}")
        return document_record
    
    def build_data_room_index(self, folder_id: str, output_path: str = None) -> str:
        """Build a complete data room index from a Google Drive folder.
        
        This is the main entry point for the indexing pipeline. It processes
        all documents in a folder and compiles them into a formatted index
        that can be used by the legal analysis agent.
        
        Args:
            folder_id: Google Drive folder ID containing the data room documents
            output_path: Optional path to save the index. If not provided,
                        saves to working_dir/data_room_index.txt
        
        Returns:
            The formatted data room index as a string
        """
        print(f"\n{'='*70}")
        print(f"BUILDING DATA ROOM INDEX")
        print(f"{'='*70}\n")
        
        # Step 1: List all files in the folder
        print("Step 1: Listing files in Google Drive folder...")
        files = self.drive_client.list_folder_contents(folder_id)
        print(f"Found {len(files)} files to process\n")
        
        # Step 2: Process each document
        print("Step 2: Processing documents...")
        document_records = []
        for idx, file in enumerate(files, start=1):
            print(f"\nProcessing file {idx}/{len(files)}")
            record = self.process_document(
                file['id'],
                file['name'],
                file['mimeType']
            )
            if record:
                document_records.append(record)
        
        # Step 3: Format the index
        print(f"\n{'='*70}")
        print("Step 3: Formatting data room index...")
        print(f"{'='*70}\n")
        
        index_lines = ["# Data Room Index\n"]
        
        # Group documents by category based on filename patterns
        # This creates a more organized index structure
        categories = self._categorize_documents(document_records)
        
        for category, docs in categories.items():
            index_lines.append(f"\n## {category}\n")
            for doc in docs:
                index_lines.append(f"- **{doc['doc_id']}**: {doc['file_name']}")
                index_lines.append(f"  Summary: {doc['document_summary']}\n")
        
        index_text = "\n".join(index_lines)
        
        # Save the index
        if output_path is None:
            output_path = self.working_dir / "data_room_index.txt"
        
        with open(output_path, 'w') as f:
            f.write(index_text)
        
        print(f"Data room index saved to: {output_path}")
        print(f"Total documents indexed: {len(document_records)}")
        
        return index_text
    
    def _categorize_documents(self, documents: List[Dict]) -> Dict[str, List[Dict]]:
        """Categorize documents based on filename patterns.
        
        This helper method groups documents into categories like contracts,
        regulatory, financial, etc. based on keywords in their filenames.
        This makes the index more navigable for the legal analyst.
        """
        categories = {
            "Corporate Documents": [],
            "Contracts": [],
            "Regulatory & Compliance": [],
            "Financial": [],
            "Legal & Litigation": [],
            "Intellectual Property": [],
            "Other": []
        }
        
        # Keywords for categorization
        category_keywords = {
            "Corporate Documents": ['incorporation', 'bylaws', 'minutes', 'charter', 'articles'],
            "Contracts": ['agreement', 'contract', 'msa', 'sow', 'purchase', 'license'],
            "Regulatory & Compliance": ['compliance', 'regulatory', 'license', 'permit', 'filing', 'sec'],
            "Financial": ['financial', 'audit', 'balance', 'income', 'statement', 'budget'],
            "Legal & Litigation": ['litigation', 'lawsuit', 'settlement', 'complaint', 'legal'],
            "Intellectual Property": ['patent', 'trademark', 'copyright', 'ip', 'intellectual']
        }
        
        for doc in documents:
            filename_lower = doc['file_name'].lower()
            categorized = False
            
            for category, keywords in category_keywords.items():
                if any(keyword in filename_lower for keyword in keywords):
                    categories[category].append(doc)
                    categorized = True
                    break
            
            if not categorized:
                categories["Other"].append(doc)
        
        # Remove empty categories
        return {k: v for k, v in categories.items() if v}


# ============================================================================
# USAGE EXAMPLE
# ============================================================================

def main():
    """Example usage of the data room indexing system."""
    
    # Configuration
    GOOGLE_CREDENTIALS_PATH = "path/to/your/google-credentials.json"
    GOOGLE_DRIVE_FOLDER_ID = "your_folder_id_here"
    OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
    
    # Initialize the indexer
    indexer = DataRoomIndexer(
        google_credentials_path=GOOGLE_CREDENTIALS_PATH,
        openai_api_key=OPENAI_API_KEY,
        working_dir="./data_room_processing"
    )
    
    # Build the complete index
    index_text = indexer.build_data_room_index(
        folder_id=GOOGLE_DRIVE_FOLDER_ID,
        output_path="./data_room_index.txt"
    )
    
    print("\n" + "="*70)
    print("INDEX PREVIEW")
    print("="*70)
    print(index_text[:1000])  # Show first 1000 characters
    print("...")


if __name__ == "__main__":
    main()
```

## Understanding the Design Decisions

Now that you have seen the complete implementation, let me explain the reasoning behind several key design choices that make this system both efficient and effective.

### Why GPT-5 Nano Throughout

You might wonder why we use GPT-5 nano for all the summarization tasks rather than more powerful models. The answer comes down to task complexity and economics. Page-level summarization is fundamentally an extraction and description task rather than a reasoning task. The model needs to identify what content appears on the page and describe it clearly, but it does not need to make complex inferences or legal judgments. GPT-5 nano excels at this type of well-defined, high-throughput work while offering the best cost-per-token ratio in the GPT-5 family.

When you consider that a typical data room might contain fifty documents averaging twenty pages each, you are processing one thousand page images. At higher reasoning levels or with more expensive models, your indexing costs would become prohibitive. GPT-5 nano allows you to build comprehensive indexes economically while reserving your budget for the actual legal analysis where reasoning matters.

### The Hierarchical Summarization Strategy

The three-level summarization hierarchy (pages to document to index) serves multiple important purposes. At the page level, you capture granular detail that might be lost in a document-level summary. Legal documents often contain critical information buried in specific clauses or footnotes, and page-level analysis ensures nothing important gets overlooked during aggregation.

At the document level, you gain context and coherence. The model can identify themes that span multiple pages, recognize how different sections relate to each other, and provide a unified narrative about what the document represents. This synthesis is valuable because documents are designed to be read as wholes, and understanding flows from seeing how pieces fit together.

At the index level, you create the navigational structure that helps your legal analyst agent decide which documents merit detailed examination. The categorization and summary descriptions allow the agent to prioritize its work based on document types and apparent relevance rather than processing everything blindly.

### Image Quality and Token Costs

The DPI setting of two hundred represents a carefully considered balance. Lower DPI values like one hundred or one hundred fifty save on file size and reduce token costs, but they risk making small text illegible, which degrades summarization quality. Higher DPI values like three hundred or four hundred provide excellent readability but dramatically increase both storage requirements and API costs because larger images consume more tokens.

Two hundred DPI provides crisp text that is easily readable while keeping image dimensions reasonable. For most business documents with standard font sizes, this resolution gives the vision model everything it needs without wasteful over-specification.

### Integration Points with Your Legal Analysis System

The data room index produced by this system plugs directly into your legal risk analysis agent as the Data Room Index input. The formatting matches exactly what your main agent expects: categorized documents with IDs and summary descriptions. Your document analyst subagent can then use the document IDs to request detailed analysis through the get_document and get_document_pages tools.

This separation of concerns is architecturally important. The indexing pipeline runs once (or whenever documents change), producing a stable index that can support multiple analysis runs. You avoid re-processing images every time you want to analyze the data room, which would be both slow and expensive. The index serves as a cached, structured representation of your raw documents optimized for agent consumption.

## Cost Optimization Strategies

When running this indexing pipeline on real data rooms, you will want to consider several optimization strategies that can significantly reduce costs without compromising quality.

First, implement parallel processing for page analysis. The page summarization tasks are completely independent of each other, making them ideal candidates for concurrent execution. You can analyze multiple pages simultaneously using threading or async programming, dramatically reducing total processing time. The OpenAI API supports high request-per-minute limits, so you can safely parallelize extensively.

Second, consider implementing smart caching. If you re-process a data room where most documents haven't changed, you can skip reanalysis of unchanged documents by comparing file modification timestamps or checksums. Store document records with their analysis results, and only update entries when source files change.

Third, tune your DPI settings based on document types. Financial documents with tables might benefit from slightly higher DPI, while simple text contracts work fine at lower resolutions. You could add logic to adjust DPI dynamically based on detected content types.

Fourth, monitor your token usage carefully. The vision API charges based on image tokens, which depend on image dimensions. You can use the token calculation formulas from the OpenAI documentation to predict costs before processing large batches, helping you budget appropriately and identify opportunities for optimization.

## Extending the System

This indexing pipeline provides a solid foundation that you can extend in several directions depending on your specific needs. You might add OCR preprocessing for documents with handwritten annotations or poor-quality scans, using libraries like Tesseract before sending to the vision API. You could implement quality checks that verify summaries contain expected information and flag documents that may have been poorly analyzed for human review.

For very large data rooms, you might implement distributed processing using task queues like Celery or cloud functions, allowing you to process hundreds of documents in parallel across multiple machines. You could add real-time monitoring and progress tracking, providing dashboards that show indexing progress and identify problematic documents.

You might also enhance the categorization logic to be more sophisticated, potentially using an additional LLM call to classify documents rather than relying on keyword matching. Or you could implement multi-language support for international data rooms where documents appear in various languages.

The system as built gives you everything needed to transform raw Google Drive folders into structured indexes ready for legal analysis. The architecture is modular, making it straightforward to swap components or add enhancements as your requirements evolve. You now have the preprocessing pipeline that feeds your legal risk analysis agent with the structured, summarized information it needs to conduct thorough due diligence efficiently.